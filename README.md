
# ğŸ•·ï¸ Async Web Crawling System â€” FastAPI + RabbitMQ + Workers

Sistema assÃ­ncrono de coleta de dados com scraping distribuÃ­do, filas de mensagens, processamento em background e API REST.

Este projeto implementa um pipeline completo de data collection com arquitetura baseada em jobs.

---

# ğŸš€ VisÃ£o Geral

A aplicaÃ§Ã£o permite:

- Coletar dados de mÃºltiplas fontes web
- Gerenciar jobs assÃ­ncronos via RabbitMQ
- Processar scraping em workers distribuÃ­dos
- Persistir resultados em PostgreSQL
- Consultar status e resultados via API REST
- Executar testes automatizados com Testcontainers
- Executar ambiente completo via Docker Compose
- Rodar pipeline CI com GitHub Actions

---

# ğŸ§  Arquitetura

```

Client â†’ FastAPI â†’ RabbitMQ â†’ Worker â†’ PostgreSQL

```

## Fluxo

1. API agenda job (`POST /crawl/*`)
2. Job Ã© salvo no banco
3. Mensagem Ã© publicada no RabbitMQ
4. Worker consome mensagem
5. Worker executa scraper
6. Resultados sÃ£o persistidos
7. Job Ã© atualizado para `completed`

---

# ğŸ“¦ Stack TecnolÃ³gica

- FastAPI â€” API assÃ­ncrona
- SQLAlchemy â€” ORM
- PostgreSQL â€” persistÃªncia
- RabbitMQ â€” message queue
- aio-pika â€” client RabbitMQ async
- httpx â€” HTTP client async
- BeautifulSoup â€” parsing HTML
- Docker + Docker Compose â€” containerizaÃ§Ã£o
- pytest â€” testes
- Testcontainers â€” integraÃ§Ã£o com DB real
- GitHub Actions â€” CI

---

# ğŸ§© Arquitetura do CÃ³digo

```

app/
â”œâ”€â”€ api/                # rotas FastAPI
â”œâ”€â”€ core/               # configuraÃ§Ãµes
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ models/         # entidades de domÃ­nio
â”‚   â”œâ”€â”€ repositories/   # acesso a dados
â”‚   â””â”€â”€ services/       # lÃ³gica de negÃ³cio
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ messaging/      # RabbitMQ
â”‚   â””â”€â”€ scrapers/
â”œâ”€â”€ workers/            # processamento background

````

### SeparaÃ§Ã£o clara de responsabilidades:

- **domain** â†’ regras de negÃ³cio
- **infrastructure** â†’ I/O externo
- **api** â†’ interface HTTP
- **workers** â†’ processamento assÃ­ncrono

---

# ğŸ¯ Sites Coletados

## Hockey Teams
- HTML com paginaÃ§Ã£o
- Parsing via BeautifulSoup

Dados:
- team_name
- year
- wins/losses
- win_pct
- goals_for / against

## Oscar Films
- Endpoint AJAX
- Parsing JSON

Dados:
- year
- title
- nominations
- awards
- best_picture

---

# âš™ï¸ Como Rodar

## Requisitos
- Docker
- Docker Compose

## Subir ambiente completo

```bash
docker compose up --build
````

ServiÃ§os iniciados:

* API â†’ [http://localhost:8000](http://localhost:8000)
* RabbitMQ UI â†’ [http://localhost:15672](http://localhost:15672)
* PostgreSQL â†’ localhost:5432

---

# ğŸ”Œ API Endpoints

## VocÃª pode verificar os endpoints disponÃ­veis em [http://localhost:8000/docs](http://localhost:8000/docs) (Swagger UI).


# ğŸ§¾ PersistÃªncia e Auditoria

## Jobs

Cada execuÃ§Ã£o possui:

* type
* error_message
* created_at

Estados:

```
pending â†’ running â†’ completed | failed
```

---

## Job Results (Auditoria)

Foi implementado histÃ³rico de execuÃ§Ã£o.

Cada linha registra:

* job_id
* data

Isso permite:

* rastreabilidade
* debugging
* reprocessamento
* observabilidade

---

# ğŸ§ª Testes

## Rodar testes

```bash
pytest -v
```

### Cobertura:

* API endpoints
* criaÃ§Ã£o de jobs
* status de jobs
* persistÃªncia
* integraÃ§Ã£o com PostgreSQL real via Testcontainers
* mocking do RabbitMQ publish

Testes nÃ£o executam scraping real (boa prÃ¡tica).

---

# ğŸ³ Docker

Ambiente completo:

* API container
* Worker container
* PostgreSQL
* RabbitMQ

Worker roda independente da API.

---

# ğŸ”„ CI com GitHub Actions

Pipeline executa:

* instalaÃ§Ã£o dependÃªncias
* lint
* testes unitÃ¡rios
* testes de integraÃ§Ã£o
* build Docker image

## Sobre push para Google Container Registry

O push para GCR nÃ£o foi incluÃ­do por dois motivos:

* requer credenciais especÃ­ficas do projeto
* nÃ£o Ã© possÃ­vel publicar imagens externas em ambiente de avaliaÃ§Ã£o

A configuraÃ§Ã£o Ã© trivial (auth + docker push) e pode ser adicionada facilmente.

---

# ğŸ§± DecisÃµes Arquiteturais

## Jobs desacoplados dos dados

Dados coletados nÃ£o possuem relaÃ§Ã£o direta com Job.

Motivos:

* evitar duplicaÃ§Ã£o massiva
* permitir reprocessamento
* manter histÃ³rico de execuÃ§Ã£o separado
* melhorar performance

Auditoria Ã© feita via `job_results`.

---

## Bulk insert

Uso de `bulk_insert_mappings` para:

* performance
* baixo overhead ORM
* melhor throughput

---

## Worker isolado

Worker roda como processo independente:

* horizontal scaling
* resiliente a falhas da API
* processamento paralelo

---

## Retry e timeout nos scrapers

Implementado:

* retry
* timeout HTTP
* parsing resiliente
* fallback seguro

---

# ğŸ”® Melhorias Futuras (Roadmap Arquitetural)

## Observabilidade

* structured logging (JSON logs)
* OpenTelemetry tracing
* metrics com Prometheus
* dashboard Grafana

## Robustez

* retry com backoff no worker
* dead letter queue
* idempotÃªncia de jobs
* deduplicaÃ§Ã£o de dados

## Performance

* batch processing configurÃ¡vel
* streaming insert
* cache de scraping

## Escalabilidade

* autoscaling de workers
* partiÃ§Ã£o de filas
* Kubernetes deployment

## API

* paginaÃ§Ã£o de resultados
* filtros
* rate limiting
* autenticaÃ§Ã£o

## Infraestrutura

* deploy em Kubernetes
* Terraform provisioning
* CI/CD com deploy automÃ¡tico

## Scraping

* suporte a Selenium
* detecÃ§Ã£o de bloqueios
* rotating proxies

---

# ğŸ‘¨â€ğŸ’» Autor

Matheus Kolln
